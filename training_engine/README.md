# è®­ç»ƒå¼•æ“ - Training Engine

## æ¨¡å—æ¦‚è¿°

è®­ç»ƒå¼•æ“æ¨¡å—æ˜¯æ•´ä¸ªæ·±åº¦å­¦ä¹ å¹³å°çš„æ ¸å¿ƒè®­ç»ƒç»„ä»¶ï¼Œæä¾›å®Œæ•´çš„è®­ç»ƒå¾ªç¯ç®¡ç†ã€ä¼˜åŒ–å™¨é…ç½®ã€æŸå¤±å‡½æ•°è®¾ç½®å’Œå­¦ä¹ ç‡è°ƒåº¦ç­‰åŠŸèƒ½ã€‚ä¸“ä¸ºç§‘å­¦è®¡ç®—å’Œå·¥ç¨‹åº”ç”¨ä¼˜åŒ–ï¼Œæ”¯æŒGPU/CPUè‡ªåŠ¨é€‚é…ã€è‡ªå®šä¹‰é¢„å¤„ç†ã€æ—©åœæœºåˆ¶ã€æ£€æŸ¥ç‚¹ç³»ç»Ÿå’ŒéªŒè¯é—´éš”é…ç½®ã€‚

## æ–°åŠŸèƒ½ç‰¹æ€§ âœ¨

### ğŸ”¥ æ–°å¢åŠŸèƒ½ (2025å¹´9æœˆæ›´æ–°)
- **æ—©åœæœºåˆ¶ (Early Stopping)** - åŸºäºéªŒè¯æŒ‡æ ‡çš„è‡ªåŠ¨åœæ­¢è®­ç»ƒ
- **æ£€æŸ¥ç‚¹ç³»ç»Ÿ (Checkpoint System)** - å®šæœŸä¿å­˜æ¨¡å‹çŠ¶æ€
- **æœ€ä½³æ¨¡å‹ä¿å­˜** - åŸºäºéªŒè¯æŸå¤±/å‡†ç¡®ç‡è‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹
- **éªŒè¯é—´éš”é…ç½®** - çµæ´»æ§åˆ¶éªŒè¯é¢‘ç‡
- **è®­ç»ƒæ¢å¤** - ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
- **æ··åˆç²¾åº¦è®­ç»ƒ** - åŠ é€Ÿè®­ç»ƒå¹¶å‡å°‘å†…å­˜ä½¿ç”¨
- **æ¢¯åº¦è£å‰ª** - é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
- **æ¨¡å‹ç¼–è¯‘** - PyTorch 2.0+ ç¼–è¯‘ä¼˜åŒ–

## å®é™…æ–‡ä»¶ç»“æ„

```
training_engine/
â”œâ”€â”€ README.md                          # æœ¬è¯´æ˜æ–‡æ¡£
â””â”€â”€ src/
    â”œâ”€â”€ training_engine.py            # è®­ç»ƒå¼•æ“æ ¸å¿ƒå®ç°
    â””â”€â”€ __init__.py                   # æ¨¡å—åˆå§‹åŒ–æ–‡ä»¶
```

## æ ¸å¿ƒåŠŸèƒ½

### 1. è®­ç»ƒå¼•æ“ç±» (TrainingEngine)
**ä½ç½®**: `src/training_engine.py`

**ä¸»è¦åŠŸèƒ½**:
- å®Œæ•´çš„è®­ç»ƒå¾ªç¯ç®¡ç†
- å¤šä¼˜åŒ–å™¨æ”¯æŒï¼ˆAdam, SGD, AdamWï¼‰
- ä¸°å¯ŒæŸå¤±å‡½æ•°ï¼ˆMSE, CrossEntropy, L1, BCEç­‰ï¼‰
- å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆStep, Exponential, CosineAnnealingï¼‰
- GPU/CPUè‡ªåŠ¨è®¾å¤‡é€‰æ‹©
- æ¨¡å‹ä¿å­˜å’ŒåŠ è½½
- è‡ªå®šä¹‰é¢„å¤„ç†å‡½æ•°æ”¯æŒ
- è®­ç»ƒå’ŒéªŒè¯æŒ‡æ ‡è·Ÿè¸ª
- å†…å­˜ä¼˜åŒ–å’Œæ¢¯åº¦ç´¯ç§¯

**ä½¿ç”¨ç¤ºä¾‹**:
```python
from src.training_engine import TrainingEngine

# åŸºç¡€é…ç½®
config = {
    "epochs": 100,
    "device": "cuda",
    "save_every": 10,
    "early_stopping": True
}

# åˆ›å»ºè®­ç»ƒå¼•æ“
trainer = TrainingEngine(config)
trainer.set_model(model)
trainer.configure_optimizer("adam", lr=0.001, weight_decay=1e-4)
trainer.configure_criterion("mse")
trainer.configure_scheduler("step", step_size=30, gamma=0.1)

# å¼€å§‹è®­ç»ƒ
history = trainer.train(train_loader, test_loader)
```

### 2. ä¼˜åŒ–å™¨é…ç½®
**æ”¯æŒçš„ä¼˜åŒ–å™¨**:
- **Adam**: è‡ªé€‚åº”çŸ©ä¼°è®¡ä¼˜åŒ–å™¨
- **SGD**: éšæœºæ¢¯åº¦ä¸‹é™
- **AdamW**: æƒé‡è¡°å‡Adam

**é…ç½®ç¤ºä¾‹**:
```python
# Adamä¼˜åŒ–å™¨
trainer.configure_optimizer("adam", lr=0.001, betas=(0.9, 0.999))

# SGDä¼˜åŒ–å™¨
trainer.configure_optimizer("sgd", lr=0.01, momentum=0.9)

# AdamWä¼˜åŒ–å™¨
trainer.configure_optimizer("adamw", lr=0.001, weight_decay=0.01)
```

### 3. æŸå¤±å‡½æ•°
**æ”¯æŒçš„æŸå¤±å‡½æ•°**:
- **MSELoss**: å‡æ–¹è¯¯å·®æŸå¤±
- **CrossEntropyLoss**: äº¤å‰ç†µæŸå¤±
- **L1Loss**: L1æŸå¤±
- **BCELoss**: äºŒå…ƒäº¤å‰ç†µæŸå¤±
- **BCEWithLogitsLoss**: å¸¦logitsçš„äºŒå…ƒäº¤å‰ç†µ

**é…ç½®ç¤ºä¾‹**:
```python
# MSEæŸå¤±
trainer.configure_criterion("mse")

# äº¤å‰ç†µæŸå¤±
trainer.configure_criterion("crossentropy")

# L1æŸå¤±
trainer.configure_criterion("l1")
```

### 4. å­¦ä¹ ç‡è°ƒåº¦å™¨
**æ”¯æŒçš„è°ƒåº¦å™¨**:
- **StepLR**: æ­¥é•¿è°ƒåº¦å™¨
- **ExponentialLR**: æŒ‡æ•°è¡°å‡
- **CosineAnnealingLR**: ä½™å¼¦é€€ç«
- **ReduceLROnPlateau**: åŸºäºéªŒè¯æŸå¤±çš„è‡ªé€‚åº”è°ƒåº¦

**é…ç½®ç¤ºä¾‹**:
```python
# Stepè°ƒåº¦å™¨
trainer.configure_scheduler("step", step_size=30, gamma=0.1)

# æŒ‡æ•°è°ƒåº¦å™¨
trainer.configure_scheduler("exponential", gamma=0.95)

# ä½™å¼¦é€€ç«
trainer.configure_scheduler("cosine", T_max=50)
```

### 5. è‡ªå®šä¹‰é¢„å¤„ç†
**åŠŸèƒ½**: æ”¯æŒå¤–éƒ¨Pythonæ–‡ä»¶å®šä¹‰è‡ªå®šä¹‰é¢„å¤„ç†å‡½æ•°

**ä½¿ç”¨ç¤ºä¾‹**:
```python
# åˆ›å»ºè‡ªå®šä¹‰é¢„å¤„ç†æ–‡ä»¶ my_preprocess.py
def preprocess_fn(data):
    """è‡ªå®šä¹‰æ•°æ®é¢„å¤„ç†"""
    if isinstance(data, dict):
        # å¤„ç†å¤šæºæ•°æ®
        combined = torch.cat([data[key] for key in sorted(data.keys())], dim=1)
        return combined
    return data

# åœ¨è®­ç»ƒå¼•æ“ä¸­ä½¿ç”¨
trainer.set_custom_preprocess("my_preprocess.py")
```

## é«˜çº§åŠŸèƒ½

### 1. æ¨¡å‹ä¿å­˜å’ŒåŠ è½½
```python
# è®¾ç½®ä¿å­˜é…ç½®
config = {
    "model_path": "models/best_model.pth",
    "save_every": 10,
    "save_best_only": True
}

# è‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹
trainer = TrainingEngine(config)

# æ‰‹åŠ¨ä¿å­˜æ¨¡å‹
trainer.save_model("models/manual_save.pth")

# åŠ è½½æ¨¡å‹
trainer.load_model("models/best_model.pth")
```

### 2. è®­ç»ƒç›‘æ§
```python
# è·å–è®­ç»ƒå†å²
history = trainer.get_training_history()
print(f"è®­ç»ƒæŸå¤±: {history['train_loss']}")
print(f"éªŒè¯æŸå¤±: {history['val_loss']}")
print(f"å­¦ä¹ ç‡: {history['learning_rate']}")
```

### 3. å†…å­˜ä¼˜åŒ–
```python
# å¯ç”¨æ¢¯åº¦ç´¯ç§¯
trainer.enable_gradient_accumulation(accumulation_steps=4)

# å†…å­˜æ¸…ç†
trainer.clear_memory()
```

### 4. æ—©åœæœºåˆ¶
```python
# é…ç½®æ—©åœ
config = {
    "early_stopping": True,
    "patience": 10,
    "min_delta": 0.001
}
```

## é…ç½®å‚æ•°è¯¦è§£

### ä¸»è¦é…ç½®é¡¹
```yaml
training:
  # åŸºæœ¬è®­ç»ƒå‚æ•°
  epochs: 100                           # è®­ç»ƒè½®æ•°ï¼Œé»˜è®¤å€¼: 10
  device: "cuda"                        # è®­ç»ƒè®¾å¤‡ ("cpu" æˆ– "cuda")ï¼Œé»˜è®¤å€¼: "cpu"
  preprocess_fn: "custom_preprocess.py" # è‡ªå®šä¹‰é¢„å¤„ç†å‡½æ•°æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤å€¼: None
  
  # æ¨¡å‹ä¿å­˜
  model_path: "models/trained_model.pth" # æ¨¡å‹ä¿å­˜è·¯å¾„ï¼Œé»˜è®¤å€¼: "trained_model.pth"
  save_every: 10                        # æ¯Nè½®ä¿å­˜ä¸€æ¬¡æ¨¡å‹ï¼Œé»˜è®¤å€¼: 1
  save_best_only: true                  # ä»…ä¿å­˜æœ€ä½³æ¨¡å‹ï¼Œé»˜è®¤å€¼: False
  
  # æ—©åœè®¾ç½®
  early_stopping: true                  # æ˜¯å¦å¯ç”¨æ—©åœï¼Œé»˜è®¤å€¼: False
  patience: 10                          # æ—©åœå®¹å¿è½®æ•°ï¼Œé»˜è®¤å€¼: 10
  min_delta: 0.001                      # æ—©åœæœ€å°æ”¹å–„å€¼ï¼Œé»˜è®¤å€¼: 0.001
  
  # æ¢¯åº¦ç´¯ç§¯
  gradient_accumulation_steps: 1        # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼Œé»˜è®¤å€¼: 1
  
  # æ—¥å¿—è®¾ç½®
  log_every: 10                         # æ¯Nè½®è®°å½•ä¸€æ¬¡æ—¥å¿—ï¼Œé»˜è®¤å€¼: 10
  verbose: true                         # æ˜¯å¦è¾“å‡ºè¯¦ç»†æ—¥å¿—ï¼Œé»˜è®¤å€¼: True

  # ä¼˜åŒ–å™¨é…ç½®
  optimizer:
    name: "adam"                        # ä¼˜åŒ–å™¨åç§° ("adam", "sgd", "adamw")ï¼Œé»˜è®¤å€¼: "adam"
    parameters:                         # ä¼˜åŒ–å™¨å‚æ•°ï¼Œé»˜è®¤å€¼: {}
      lr: 0.001                         # å­¦ä¹ ç‡ï¼Œé»˜è®¤å€¼: æ ¹æ®ä¼˜åŒ–å™¨è€Œå®š
      weight_decay: 1e-4                # æƒé‡è¡°å‡ï¼Œé»˜è®¤å€¼: 0
      betas: [0.9, 0.999]               # Adamä¼˜åŒ–å™¨å‚æ•°ï¼Œé»˜è®¤å€¼: [0.9, 0.999]

  # æŸå¤±å‡½æ•°é…ç½®
  criterion:
    name: "mse"                         # æŸå¤±å‡½æ•°åç§° ("cross_entropy", "mse", "l1", "bce", "bce_with_logits")ï¼Œé»˜è®¤å€¼: "cross_entropy"
    parameters: {}                      # æŸå¤±å‡½æ•°å‚æ•°ï¼Œé»˜è®¤å€¼: {}

  # å­¦ä¹ ç‡è°ƒåº¦å™¨é…ç½®
  scheduler:
    name: "step"                        # è°ƒåº¦å™¨åç§° ("step", "exponential", "cosine")ï¼Œé»˜è®¤å€¼: None
    parameters:                         # è°ƒåº¦å™¨å‚æ•°ï¼Œé»˜è®¤å€¼: {}
      step_size: 30                     # Stepè°ƒåº¦å™¨å‚æ•°ï¼Œé»˜è®¤å€¼: None
      gamma: 0.1                        # è¡°å‡ç‡ï¼Œé»˜è®¤å€¼: None
```

### é…ç½®å‚æ•°è¯¦ç»†è¯´æ˜

| å‚æ•°å | ç±»å‹ | å¿…éœ€ | é»˜è®¤å€¼ | æè¿° |
|-------|------|------|--------|------|
| epochs | int | å¯é€‰ | 10 | è®­ç»ƒè½®æ•° |
| device | str | å¯é€‰ | "cpu" | è®­ç»ƒè®¾å¤‡ ("cpu" æˆ– "cuda") |
| preprocess_fn | str | å¯é€‰ | None | è‡ªå®šä¹‰é¢„å¤„ç†å‡½æ•°æ–‡ä»¶è·¯å¾„ |
| model_path | str | å¯é€‰ | "trained_model.pth" | æ¨¡å‹ä¿å­˜è·¯å¾„ |
| save_every | int | å¯é€‰ | 1 | æ¯Nè½®ä¿å­˜ä¸€æ¬¡æ¨¡å‹ |
| save_best_only | bool | å¯é€‰ | False | ä»…ä¿å­˜æœ€ä½³æ¨¡å‹ |
| early_stopping | bool | å¯é€‰ | False | æ˜¯å¦å¯ç”¨æ—©åœ |
| patience | int | å¯é€‰ | 10 | æ—©åœå®¹å¿è½®æ•° |
| min_delta | float | å¯é€‰ | 0.001 | æ—©åœæœ€å°æ”¹å–„å€¼ |
| gradient_accumulation_steps | int | å¯é€‰ | 1 | æ¢¯åº¦ç´¯ç§¯æ­¥æ•° |
| log_every | int | å¯é€‰ | 10 | æ¯Nè½®è®°å½•ä¸€æ¬¡æ—¥å¿— |
| verbose | bool | å¯é€‰ | True | æ˜¯å¦è¾“å‡ºè¯¦ç»†æ—¥å¿— |
| optimizer.name | str | å¯é€‰ | "adam" | ä¼˜åŒ–å™¨åç§° ("adam", "sgd", "adamw") |
| optimizer.parameters | dict | å¯é€‰ | {} | ä¼˜åŒ–å™¨å‚æ•° |
| criterion.name | str | å¯é€‰ | "cross_entropy" | æŸå¤±å‡½æ•°åç§° ("cross_entropy", "mse", "l1", "bce", "bce_with_logits") |
| criterion.parameters | dict | å¯é€‰ | {} | æŸå¤±å‡½æ•°å‚æ•° |
| scheduler.name | str | å¯é€‰ | None | è°ƒåº¦å™¨åç§° ("step", "exponential", "cosine") |
| scheduler.parameters | dict | å¯é€‰ | {} | è°ƒåº¦å™¨å‚æ•° |

## æµ‹è¯•éªŒè¯

### æµ‹è¯•æ–¹æ³•
è®­ç»ƒå¼•æ“çš„æµ‹è¯•ä½äºä¸Šçº§ç›®å½•çš„ `test/training_engine/` ä¸­ï¼š
- `test/training_engine/test_training_engine.py` - è®­ç»ƒå¼•æ“æ ¸å¿ƒæµ‹è¯•
- `test/training_engine/test_custom_preprocessing.py` - è‡ªå®šä¹‰é¢„å¤„ç†æµ‹è¯•

### è¿è¡Œæµ‹è¯•
```bash
# è¿è¡Œè®­ç»ƒå¼•æ“æµ‹è¯•
python -m pytest test/training_engine/ -v

# è¿è¡Œç‰¹å®šæµ‹è¯•
python -m pytest test/training_engine/test_training_engine.py::test_optimizer_config -v
```

## ä½¿ç”¨ç¤ºä¾‹

### 1. å®Œæ•´è®­ç»ƒæµç¨‹
```python
from training_engine.src.training_engine import TrainingEngine

# é…ç½®è®­ç»ƒ
config = {
    "epochs": 50,
    "device": "cuda",
    "model_path": "models/fno_model.pth",
    "save_best_only": True,
    "early_stopping": True,
    "patience": 15
}

# åˆ›å»ºè®­ç»ƒå™¨
trainer = TrainingEngine(config)

# è®¾ç½®æ¨¡å‹å’Œæ•°æ®
trainer.set_model(model)
trainer.set_data_loaders(train_loader, test_loader)

# é…ç½®è®­ç»ƒç»„ä»¶
trainer.configure_optimizer("adam", lr=0.001, weight_decay=1e-4)
trainer.configure_criterion("mse")
trainer.configure_scheduler("cosine", T_max=50)

# å¼€å§‹è®­ç»ƒ
history = trainer.train()

# ä¿å­˜è®­ç»ƒå†å²
import json
with open("results/training_history.json", "w") as f:
    json.dump(history, f)
```

### 2. æ¢å¤è®­ç»ƒ
```python
# ä»æ£€æŸ¥ç‚¹æ¢å¤
trainer.load_checkpoint("checkpoints/epoch_25.pth")
trainer.train(resume_from=26)
```

### 3. å¤šGPUè®­ç»ƒ
```python
# è‡ªåŠ¨å¤šGPUæ”¯æŒ
config = {
    "device": "cuda",
    "multi_gpu": True,
    "distributed": False
}
```

## å¼€å‘è®¡åˆ’

### çŸ­æœŸè®¡åˆ’ï¼ˆ1ä¸ªæœˆå†…ï¼‰
1. **åŠŸèƒ½å¢å¼º**
   - æ·»åŠ æ··åˆç²¾åº¦è®­ç»ƒæ”¯æŒ
   - å®ç°æ¢¯åº¦è£å‰ªåŠŸèƒ½
   - æ·»åŠ å­¦ä¹ ç‡çƒ­èº«

2. **ç›‘æ§æ”¹è¿›**
   - é›†æˆWeights & Biasesæ—¥å¿—
   - æ·»åŠ TensorBoardæ”¯æŒ
   - å®ç°å®æ—¶è®­ç»ƒå¯è§†åŒ–

3. **æ£€æŸ¥ç‚¹ç³»ç»Ÿ**
   - æ·»åŠ è®­ç»ƒä¸­æ–­æ¢å¤
   - å®ç°å¢é‡ä¿å­˜
   - æ”¯æŒæ–­ç‚¹ç»­è®­

### ä¸­é•¿æœŸè®¡åˆ’ï¼ˆ3-6ä¸ªæœˆï¼‰
1. **é«˜çº§åŠŸèƒ½**
   - æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒ
   - æ·»åŠ è‡ªåŠ¨è¶…å‚æ•°è°ƒä¼˜
   - å®ç°æ¨¡å‹å‰ªæ

2. **æ€§èƒ½ä¼˜åŒ–**
   - é›†æˆNVIDIA ApexåŠ é€Ÿ
   - æ·»åŠ åŠ¨æ€æ‰¹å¤§å°è°ƒæ•´
   - æ”¯æŒå¼‚æ­¥æ•°æ®åŠ è½½

3. **å®éªŒç®¡ç†**
   - æ·»åŠ å®éªŒè·Ÿè¸ªç³»ç»Ÿ
   - å®ç°æ¨¡å‹ç‰ˆæœ¬ç®¡ç†
   - æ”¯æŒA/Bæµ‹è¯•æ¡†æ¶

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜
1. **CUDAå†…å­˜ä¸è¶³**
   - å‡å°æ‰¹æ¬¡å¤§å°
   - å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
   - ä½¿ç”¨CPUè®­ç»ƒ

2. **è®­ç»ƒé€Ÿåº¦æ…¢**
   - æ£€æŸ¥æ•°æ®åŠ è½½ç“¶é¢ˆ
   - ä¼˜åŒ–æ¨¡å‹ç»“æ„
   - ä½¿ç”¨æ›´é«˜æ•ˆçš„ä¼˜åŒ–å™¨

3. **æ¨¡å‹ä¸æ”¶æ•›**
   - è°ƒæ•´å­¦ä¹ ç‡
   - æ£€æŸ¥æ•°æ®æ ‡å‡†åŒ–
   - éªŒè¯æŸå¤±å‡½æ•°é€‰æ‹©

### è°ƒè¯•å·¥å…·
```python
# æ£€æŸ¥è®­ç»ƒé…ç½®
print(trainer.get_config_summary())

# éªŒè¯æ•°æ®æµ
trainer.validate_data_loaders()

# æ£€æŸ¥è®¾å¤‡ä¿¡æ¯
print(trainer.get_device_info())

# å†…å­˜ä½¿ç”¨ç›‘æ§
trainer.enable_memory_profiling()
```

## æœ€ä½³å®è·µ

### 1. è®­ç»ƒå‡†å¤‡
- ä½¿ç”¨GPUè®­ç»ƒè·å¾—æœ€ä½³æ€§èƒ½
- åˆç†è®¾ç½®æ‰¹æ¬¡å¤§å°å¹³è¡¡é€Ÿåº¦å’Œå†…å­˜
- å¯ç”¨æ—©åœé¿å…è¿‡æ‹Ÿåˆ

### 2. è¶…å‚æ•°è°ƒä¼˜
- ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨
- å®éªŒä¸åŒçš„ä¼˜åŒ–å™¨ç»„åˆ
- ç›‘æ§éªŒè¯æŸå¤±è°ƒæ•´è®­ç»ƒ

### 3. æ¨¡å‹ä¿å­˜
- å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
- ä¿å­˜æœ€ä½³éªŒè¯æŸå¤±æ¨¡å‹
- ä¿æŒè®­ç»ƒå†å²è®°å½•

### 4. è°ƒè¯•æŠ€å·§
- å…ˆåœ¨å°æ•°æ®é›†ä¸ŠéªŒè¯
- ä½¿ç”¨ç®€å•çš„æ¨¡å‹ç»“æ„æµ‹è¯•
- é€æ­¥å¢åŠ å¤æ‚åº¦

---

*æœ€åæ›´æ–°ï¼š2025å¹´9æœˆ1æ—¥*