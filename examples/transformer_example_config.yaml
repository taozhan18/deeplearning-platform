# Example configuration for using Transformer model with the low-code platform

# Data configuration
data:
  # For demonstration, we'll use the test data generated by the platform
  train_data_path: "data/train_data.npy"
  train_targets_path: "data/train_targets.npy"
  test_data_path: "data/test_data.npy"
  test_targets_path: "data/test_targets.npy"
  # Data loading parameters
  batch_size: 32
  shuffle: true

# Model configuration
model:
  name: "transformer"
  parameters:
    # Input and output dimensions
    input_dim: 10     # Dimension of input features
    output_dim: 1     # Dimension of output features
    
    # Model architecture parameters
    d_model: 64       # The dimension of the model embeddings
    n_layers: 4       # Number of encoder/decoder layers
    n_heads: 8        # Number of attention heads
    pf_dim: 256       # Position-wise feedforward hidden dimension
    dropout: 0.1      # Dropout rate
    max_len: 100      # Maximum sequence length

# Training configuration
training:
  # Optimizer settings
  optimizer:
    name: "adam"
    parameters:
      lr: 0.001
      weight_decay: 0.0001
  
  # Loss function
  criterion:
    name: "mse"
  
  # Learning rate scheduler
  scheduler:
    name: "step_lr"
    parameters:
      step_size: 10
      gamma: 0.95
  
  # Training parameters
  epochs: 50
  device: "cuda"  # or "cpu"
  
  # Checkpoint settings
  save_checkpoint: true
  checkpoint_dir: "checkpoints"
  checkpoint_interval: 10

# Logging configuration
logging:
  log_dir: "logs"
  log_interval: 10

# Output configuration
output:
  model_save_path: "saved_models/transformer_model.pth"
  predictions_save_path: "results/transformer_predictions.npy"